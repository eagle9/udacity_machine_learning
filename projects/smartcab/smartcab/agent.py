import random
import math
import os
from environment import Agent, Environment
from planner import RoutePlanner
from simulator import Simulator


class LearningAgent(Agent):
	""" An agent that learns to drive in the Smartcab world.
		This is the object you will be modifying. """ 

	def __init__(self, env, learning=False, epsilon=1.0, alpha=0.5, epsilon_rule=1):
		super(LearningAgent, self).__init__(env)     # Set the agent in the evironment 
		self.planner = RoutePlanner(self.env, self)  # Create a route planner
		self.valid_actions = self.env.valid_actions  # The set of valid actions

		# Set parameters of the learning agent
		self.learning = learning # Whether the agent is expected to learn
		self.Q = dict()          # Create a Q-table which will be a dictionary of tuples
		self.epsilon = epsilon   # Random exploration factor
		self.alpha = alpha       # Learning factor
		self.try_cnt = 1
		self.epsilon_rule = epsilon_rule
		###########
		## TO DO ##
		###########
		# Set any additional class parameters as needed


	def reset(self, destination=None, testing=False):
		""" The reset function is called at the beginning of each trial.
			'testing' is set to True if testing trials are being used
			once training trials have completed. """

		# Select the destination as the new location to route to
		self.planner.route_to(destination)
		
		########### 
		## TO DO ##
		###########
		# Update epsilon using a decay function of your choice
		# Update additional class parameters as needed
		# If 'testing' is True, set epsilon and alpha to 0
		if testing:
			self.epsilon = 0
		elif self.epsilon_rule==1:
			self.epsilon -=0.05
		elif self.epsilon_rule==2:
			self.epsilon = 1.0/(self.try_cnt)
		elif self.epsilon_rule==3: 
			self.epsilon = 1.0/(self.try_cnt**2)
		elif self.epsilon_rule==4: 
			self.epsilon = math.e**(self.try_cnt/-10.0)
		elif self.epsilon_rule==5: 
			self.epsilon = 0.999**self.try_cnt #math.cos(self.try_cnt/3.0)
		else:
			return None
		
		self.try_cnt+=1
			
		return None

	def build_state(self):
		""" The build_state function is called when the agent requests data from the 
			environment. The next waypoint, the intersection inputs, and the deadline 
			are all features available to the agent. """

		# Collect data about the environment
		waypoint = self.planner.next_waypoint() # The next waypoint 
		inputs = self.env.sense(self)           # Visual input - intersection light and traffic
		deadline = self.env.get_deadline(self)  # Remaining deadline

		########### 
		## TO DO ##
		###########
		# Set 'state' as a tuple of relevant data for the agent        
		state = (waypoint,inputs['light'],inputs['oncoming'],inputs['left'], inputs['right'])

		return state


	def get_maxQ(self, state):
		""" The get_max_Q function is called when the agent is asked to find the
			maximum Q-value of all actions based on the 'state' the smartcab is in. """

		########### 
		## TO DO ##
		###########
		# Calculate the maximum Q-value of all actions for a given state
		
		maxQ=max(self.Q[state].values())
		
		#try:
			#qlist=[self.Q[(state,None)],self.Q[(state,'left')],self.Q[(state,'right')],self.Q[(state,'forward')]]
		#	maxQ=None#max(qlist)
		#except:
			#self.createQ(state)
		#	maxQ=None	

		return maxQ 


	def createQ(self, state):
		""" The createQ function is called when a state is generated by the agent. """

		########### 
		## TO DO ##
		###########
		# When learning, check if the 'state' is not in the Q-table
		# If it is not, create a new dictionary for that state
		#   Then, for each action available, set the initial Q-value to 0.0
		if state in self.Q:
			pass
		else:
			self.Q[state]={None:0,'left':0,'right':0,'forward':0}
		return


	def choose_action(self, state):
		""" The choose_action function is called when the agent is asked to choose
			which action to take, based on the 'state' the smartcab is in. 
			[None,'left','right','forward']) """

		# Set the agent state and default action
		self.state = state
		self.next_waypoint = self.planner.next_waypoint()
		
		#print self.next_waypoint

		if self.learning and random.random() > self.epsilon:
			
			maxQ=self.get_maxQ(state) 
			#print self.Q[state], maxQ
			cand_actions=[act for act in self.Q[state] if self.Q[state][act]==maxQ]
			
			action=random.choice(cand_actions)
			#print "MaxQ, action:", maxQ, action
                 
		else:
			action=random.choice( self.valid_actions)

		########### 
		## TO DO ##
		###########
		# When not learning, choose a random action
		# When learning, choose a random action with 'epsilon' probability
		#   Otherwise, choose an action with the highest Q-value for the current state
 
		return action


	def learn(self, state, action, reward):
		""" The learn function is called after the agent completes an action and
			receives an award. This function does not consider future rewards 
			when conducting learning. """

		########### 
		## TO DO ##
		###########
		# When learning, implement the value iteration update rule
		#   Use only the learning rate 'alpha' (do not use the discount factor 'gamma')
		#print self.Q[state][action], reward
		temp = self.Q[state][action]
		if self.learning:
			self.Q[state][action] = (1-self.alpha)*temp + self.alpha*reward
			#print self.Q[state][action]

		return


	def update(self):
		""" The update function is called when a time step is completed in the 
			environment for a given trial. This function will build the agent
			state, choose an action, receive a reward, and learn if enabled. """

		state = self.build_state()          # Get current state
		self.createQ(state)                 # Create 'state' in Q-table
		action = self.choose_action(state)  # Choose an action
		reward = self.env.act(self, action) # Receive a reward
		self.learn(state, action, reward)   # Q-learn

		return
		

def run(tolerance_,alpha_,epsilon_rule_,detail_out_=False):
	""" Driving function for running the simulation. 
		Press ESC to close the simulation, or [SPACE] to pause the simulation. """

	##############
	# Create the environment
	# Flags:
	#   verbose     - set to True to display additional output from the simulation
	#   num_dummies - discrete number of dummy agents in the environment, default is 100
	#   grid_size   - discrete number of intersections (columns, rows), default is (8, 6)
	env = Environment(verbose=False, num_dummies=100, grid_size=(8,6), detail_out=detail_out_)
	
	##############
	# Create the driving agent
	# Flags:
	#   learning   - set to True to force the driving agent to use Q-learning
	#    * epsilon - continuous value for the exploration factor, default is 1
	#    * alpha   - continuous value for the learning rate, default is 0.5
	agent = env.create_agent(LearningAgent,learning=True, epsilon=1, alpha=alpha_, epsilon_rule=epsilon_rule_)
	
	##############
	# Follow the driving agent
	# Flags:
	#   enforce_deadline - set to True to enforce a deadline metric
	env.set_primary_agent(agent, enforce_deadline=True)

	##############
	# Create the simulation
	# Flags:
	#   update_delay - continuous time (in seconds) between actions, default is 2.0 seconds
	#   display      - set to False to disable the GUI if PyGame is enabled
	#   log_metrics  - set to True to log trial and simulation results to /logs
	#   optimized    - set to True to change the default log file name
	sim = Simulator(env, update_delay=0.0, display=False, log_metrics=True, optimized=True, detail_out=detail_out_)
	
	##############
	# Run the simulator
	# Flags:
	#   tolerance  - epsilon tolerance before beginning testing, default is 0.05 
	#   n_test     - discrete number of testing trials to perform, default is 0
	print "tolerance_=",tolerance_, " alpha_=",alpha_, " epsilon_rule_=",epsilon_rule_
	sim.run(n_test=10, tolerance=tolerance_)


if __name__ == '__main__':
	os.chdir(u'C:\\GitHub\\machine-learning\\projects\\smartcab')
	
	#os.chdir('C:\\Users\\sangh\\Documents\\GitHub\\machine-learning\\projects\\smartcab')
	
	#run(tolerance_=0.0001, alpha_=0.99, epsilon_rule_=3, detail_out_=False) # A+ A
	#run(tolerance_=0.001, alpha_=0.99, epsilon_rule_=2, detail_out_=False) #A+ A+
	#run(tolerance_=0.005, alpha_=0.95, epsilon_rule_=2, detail_out_=False) #A+ A+
	#run(tolerance_=0.005, alpha_=0.65, epsilon_rule_=1, detail_out_=False) #F B
	#run(tolerance_=0.001, alpha_=0.9, epsilon_rule_=4, detail_out_=False) #A+ A   -20
	run(tolerance_=0.0000001, alpha_=0.85, epsilon_rule_=4, detail_out_=False) #A+ A+
	#run(tolerance_=0.00001, alpha_=0.95, epsilon_rule_=1, detail_out_=False) # No optimum
	#3  
